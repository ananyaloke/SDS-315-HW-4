---
title: "SDS 315 HW 3"
author: "Ananya Loke - UT EID: asl3324"
date: "2025-01-27"
output: 
  pdf_document:
    includes:
      toc: true
---


Click [here](https://github.com/ananyaloke/SDS-315-HW-4) for the Github link to the code!


```{r global_options, echo=FALSE}
knitr::opts_chunk$set(fig.height=6, fig.width=8, warning=FALSE, message = FALSE, tidy=TRUE, echo=FALSE, tidy.opts=list(width.cutoff=60))
```


```{r, echo = FALSE, include=FALSE}
#loading libraries
library(tidyverse)
library(lubridate)
library(kableExtra)
library(rvest)
library(RColorBrewer)
library(mosaic)


#reading in datasets
let_freq = read_csv("letter_frequencies.csv")



```


# **Problem 1: Iron Bank**


```{r}

set.seed(555)

sim_flagged = do(100000)*nflip(n=2021, prob=0.024)

ggplot(sim_flagged) +   
  geom_histogram(data = subset(sim_flagged, nflip > 70), 
                 aes(x = nflip), binwidth = 1, 
                 col = "purple3", fill = "lightslateblue") +
  geom_histogram(data = subset(sim_flagged, nflip <= 70), 
                 aes(x = nflip), binwidth = 1, 
                 col = "firebrick4", fill = "lightpink") +
  geom_vline(xintercept = 70, linetype = "longdash", col = "black") +
  theme_classic()



p_val1 = sum(sim_flagged >= 70)/100000





```
 

#### Null Hypothesis: The probability of a legal trade being flagged by the SEC's detection algorithm is 2.4%. 
#### Test Statistic: Of the last 2021 trades by Iron Bank employees, 70 were flagged by the SEC’s detection algorithm. 
#### P-Value: `r p_val1`
#### Conclusion: The p-value found is below 0.01 which means that, while not entirely impossible, 70 out of 2021 trades being flagged is not the same 2.4% baseline rate as that of other traders. With this low of a p-value, we can reject the null hypothesis.  




# **Problem 2: Health Inspections**


```{r}
set.seed(555)

sim_inspect = do(100000)*nflip(n=50, prob=0.03)

ggplot(sim_inspect) +   
  geom_histogram(data = subset(sim_flagged, nflip >= 8), 
                 aes(x = nflip), binwidth = 1, 
                 col = "firebrick4", fill = "lightpink") +
  geom_histogram(data = subset(sim_flagged, nflip < 8), 
                 aes(x = nflip), binwidth = 1, 
                 col = "purple3", fill = "lightslateblue") +
  geom_vline(xintercept = 8, linetype = "longdash", col = "black") +
  theme_classic()



p_val2 = sum(sim_inspect >= 8)/100000

```


#### Null Hypothesis: On average, 3% of all restaurant inspections result in health code violations due to random issues that can occur even in well-managed establishments. 
#### Test Statistic: Over the last year, Gourmet Bites was inspected a total of 50 times, 8 of which resulted in health code violations being reported. 
#### P-Value: `r p_val2`
#### Conclusion: The p-value is significantly lower than 10^-3 meaning that we can confidently reject the null hypothesis and say that the observed data for Gourmet Bites is not consistent with the Health Department’s statistic that restaurants in the city are cited for health code violations at the same 3% baseline rate.




# **Problem 3: Evaluating Jury Selection for Bias**



```{r}
observed = c(85, 56, 59, 27, 13)
expected = 240 * c(0.30, 0.25, 0.20, 0.15, 0.10)
chi_squared_stat1 = sum((observed - expected)^2 / expected)
p_val3 = pchisq(chi_squared_stat1, df = 4, lower.tail = FALSE)


```

#### For this problem, we can use a chi-square test to analyze whether the distribution of jurors empaneled by this judge is significantly different from the county’s population proportions. Our null hypothesis is that the total number of jurors (240) will be 30% Group 1 (72), 25% Group 2 (60), 20% Group 3 (48), 15% Group 4 (36), and 10% Group 5 (24). Our test statistic is a group of 240 jurors with 85 Group 1 members, 56 Group 2 members, 59 Group 3 members, 27 Group 4 members, and 14 Group 5 members. After calculating, we get a chi squared value of `r chi_squared_stat1` and a p-value of `r p_val3`. While this p-value is below 0.05, raising some questions about if the distribution matches the expected proportions, it is not low enough to confirm a significant difference from the expected proportions. The differences could be explained away with sampling variance (random chance differences in the sampled population) or sampling bias (which area of the county was sampled? did it cover the whole county evenly?). To investigate further, we could repeat the sampling process with blocking and then randomizing. However, since that may be impractical, we could also re-investigate the same sample using bootstrapping to extrapolate the data and come to a more certain conclusion about how the sample distribution compares to the true county’s population proportions.






# **Problem 4: LLM watermarking**

### Part A: The Null or Reference Distribution



```{r}

#read the sentences
sentences = readLines("brown_sentences.txt", warn = FALSE)
sentences_df <- data.frame(sentence = sentences, stringsAsFactors = FALSE)



calculate_chi_squared = function(sentence, freq_table) {
  
  # Ensure letter frequencies are normalized and sum to 1
  freq_table$Probability = freq_table$Probability / sum(freq_table$Probability)
  
  # Remove non-letters and convert to uppercase
  clean_sentence = gsub("[^A-Za-z]", "", sentence)
  clean_sentence = toupper(clean_sentence)
  
  # Count the occurrences of each letter in the sentence
  observed_freq = table(factor(strsplit(clean_sentence, "")[[1]], levels = freq_table$Letter))
  
  # Calculate expected counts
  total_letters = sum(observed_freq)
  expected_freq = total_letters * freq_table$Probability
  
  # Chi-squared statistic
  chi_squared_stat = sum((observed_freq - expected_freq)^2 / expected_freq)
  
  return(chi_squared_stat)
}


#compile the distribution
for (i in 1:length(sentences_df$sentence)) {
  sentences_df$chi_squared_val[i] = calculate_chi_squared(sentences_df$sentence[i], let_freq)
}



#view the distribution
ggplot(sentences_df, aes(x = chi_squared_val)) +   
  geom_histogram (bin = 30, col = "firebrick4", fill = "lightpink") +
  labs(title = "Chi-Square Distribution of Sentences", x = "Chi-Square Value", y = "Frequency") + 
  theme_classic()
```

#### The histogram above shows the distribution of the chi squared statistic for the sentences in the brown_sentences.txt file. 


### Part B: checking for a watermark

```{r, fig.height=5, fig.width=8}

test_sentences <- c(
  "She opened the book and started to read the first chapter, eagerly anticipating what might come next.",
  "Despite the heavy rain, they decided to go for a long walk in the park, crossing the main avenue by the fountain in the center.",
  "The museum’s new exhibit features ancient artifacts from various civilizations around the world.",
  "He carefully examined the document, looking for any clues that might help solve the mystery.",
  "The students gathered in the auditorium to listen to the guest speaker’s inspiring lecture.",
  "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland.",
  "The chef demonstrated how to prepare a delicious meal using only locally sourced ingredients, focusing mainly on some excellent dinner recipes from Spain.",
  "They watched the sunset from the hilltop, marveling at the beautiful array of colors in the sky.",
  "The committee reviewed the proposal and provided many points of useful feedback to improve the project’s effectiveness.",
  "Despite the challenges faced during the project, the team worked tirelessly to ensure its successful completion, resulting in a product that exceeded everyone’s expectations."
)

test_sentences_df <- data.frame(sentence = test_sentences)

for (i in 1:length(test_sentences_df$sentence)) {
  test_sentences_df$chi_squared_val[i] = calculate_chi_squared(test_sentences_df$sentence[i], let_freq)
  test_sentences_df$p_val[i] = mean(sentences_df$chi_squared_val >= test_sentences_df$chi_squared_val[i])
}



styled_table <- kbl(test_sentences_df, col.names = c("Sentence", "Chi-Squared Value", "P-Value"), format = "latex", booktabs = TRUE) %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE, position = "left") %>%
  column_spec(1, width = "9cm") %>%  # Adjust column width to fit within page margins
  column_spec(2, width = "3cm") %>%  # Set width for Chi-Squared Value column
  column_spec(3, width = "3cm") %>% # Set width for P-Value column
  row_spec(0, bold = TRUE)   # Make header row bold
  #kable_styling(latex_options = c("hold_position"))  # Prevent floating issues in PDF

styled_table


```
#### I believe the sentence produced by an LLM is the following sentence:


"Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland."


#### This is because the p-value is the smallest/farthest from 1 (0.0087761) and the sentence has an extremely high chi-squared value comparatively. This leads me to believe that it was not written by a human as is least follows the “typical” English letter distribution.


